## 3 Monte Carlo-Based Reinforcement Learning Method

### 3.1 基于蒙特卡洛方法的理论

强化学习算法的精髓之一是解决无模型的马尔科夫决策问题。基于值函数的无模型的强化学习算法主要包括蒙特卡洛方法和时间差分方法。

对于模型已知的马尔科夫决策过程，可以采用动态规划的方法解决，即策略迭代和值迭代。这两种方法可以用广义策略迭代方法统一：即先进行策略评估，也就是计算当前策略所对应的值函数，再利用值函数改进当前策略。无模型的强化学习基本思想也是如此，即：策略评估和策略改善。

在动态规划的方法中，值函数的计算公式为
$$
\nu_{\pi}(s) = \sum_{a \in A}\pi(a|s)(R^a_s + \gamma\sum_{s' \in S} P^a_{SS'}\nu_{\pi}(s'))
$$
动态规划方法计算状态 $s$ 处的值函数利用了模型 $P^a_{SS'}$ ，而在无模型强化学习中，模型 $P^a_{SS'}$ 是未知的。⽆模型的强化学习算法要想利⽤策略评估和策略改善的框架，必须采⽤其他的⽅法评估当前策略（计算值函数）。

我们回到值函数最原始的定义公式：
$$
\nu_{\pi}(s) = E_{\pi}[G_t|S_t=s] = E_{\pi}[\sum^{\infty}_{k=0}\gamma^k R_{t+k+1} | S_t=s]
$$

$$
q_{\pi}(s) = E_{\pi}[\sum^{\infty}_{k=0}\gamma^k R_{t+k+1} | S_t=s,A_t=a]
$$

状态值函数和⾏为值函数的计算实际上是计算返回值的期望，动态规划的⽅法是利⽤模型计算该期望。在没有模型时，我们可以采⽤蒙特卡洛的⽅法计算该期望，即利⽤随机样本估计期望。在计算值函数时，蒙特卡洛⽅法是利⽤**经验平均**代替随机变量的期望。此处，我们要理解两个词：经验和平均。

**什么是“经验”**

当要评估智能体的当前策略 $\pi$ 时，我们可以利用策略 $\pi$ 产生很多次试验，每次试验都是从任意的初始状态开始直到终止，比如一次试验为 $S_1,A_1,R_2,\cdots,S_T$ ，计算一次试验中状态 $s$ 处的折扣回报返回值为 $G_t(s) = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1} R_T$ ，那么经验就是指利用该策略做很多次试验，产生很多幕数据。如下图所示。

![](.\1.png)

**什么是“平均”**

平均就是求平均值。 利⽤蒙特卡罗⽅法求状态 $s$ 处的值函数时，⼜可以分为第⼀次访问蒙特卡洛⽅法和每次访问蒙特卡洛⽅法。

第一次访问蒙特卡洛方法是指在计算状态 $s$ 处的值函数时，只利用每次试验中第一次访问到状态 $s$ 时的返回值。如上图中第一次试验所示，计算状态 $s$ 处的均值只利用 $G_{11}$ ，因此，第一次访问蒙特卡洛方法的计算公式为
$$
\nu(s) = \frac{G_{11}(s) + G_{21}(s) + \cdots}{N(s)}
$$
每次访问蒙特卡洛方法是指在计算状态 $s$ 处的值函数，利用所有访问到状态 $s$ 时的回报返回值，即
$$
\nu(s) = \frac{G_{11}(s) + G_{12}(s) + \cdots + G_{21}(s) + \cdots}{N(s)}
$$
根据大数定律： $\nu(s) \to \nu_{\pi}(s)\ \text{as} \ N(s) \to \infty$ 。

由于智能体与环境交互的模型是未知的，蒙特卡洛⽅法是利⽤经验平均来估计值函数，⽽能否得到正确的值函数，则取决于经验——因此，如何获得充⾜的经验是⽆模型强化学习的核⼼所在。

在动态规划⽅法中，为了保证值函数的收敛性，算法会逐个扫描状态空间中的状态。⽆模型的⽅法充分评估策略值函数的前提是每个状态都能被访问到，因此，在蒙特卡洛⽅法中必须采⽤⼀定的⽅法保证每个状态都能被访问到，⽅法之⼀是**探索性初始化**。

探索性初始化是指每个状态都有一定几率作为初始状态。在学习基于探索性初始化的蒙特卡洛方法前，需要先了解策略改善方法，以及便于进行迭代计算的平均方法。

**（1）蒙特卡洛策略改善。**

蒙特卡洛方法利用经验平均估计策略值函数。估计出值函数后，对于每个状态，它通过最大化动作值函数来进行策略的改善。即 $\pi(s) = \arg \underset{a}{\text{max}} \ q(s,a)$ 。

**（2）递增计算均值的方法如下式所示。**
$$
\nu_{k}(s) = \frac{1}{k} \sum^{k}_{j=1}G_j(s) \\
		   = \frac{1}{k} (G_k(s)) + \sum^{k-1}_{j=1} G_j(s) \\
		   = \frac{1}{k} (G_k(s) + (k-1)\nu_{k-1}(s)) \\
		   = \nu_{k-1}(s) + \frac{1}{k}(G_k(s) - \nu_{k-1}(s))
$$
下图即为探索性初始化蒙特卡洛方法的伪代码。

![](.\2.png)

探索性初始化在迭代每⼀幕时，初始状态是随机分配的，这样可以保证迭代过程中每个状态⾏为对都能被选中。它蕴含着⼀个假设：假设所有的动作都被⽆限频繁选中。对于这个假设，有时很难成⽴，或⽆法完全保证。如何保证在初始状态不变的同时，⼜能保证每个状态⾏为对可以被访问到？

精心设计探索策略。策略必须是温和的，即对所有的状态 $s$ 和 $a$ 满足： $\pi(a|s)>0$ 。也就是说，温和的探索策略是指在任意状态下，采用动作集中每个动作的概率都大于零。典型的温和策略是 $\epsilon-soft$ 策略：
$$
\pi(a|s) \gets \begin{cases} 1-\epsilon+\frac{\epsilon}{|A(s)|} & if \ a = \arg  max_aQ(s,a) \\
\frac{\epsilon}{|A(s)|} & if \ a \neq \arg max_aQ(s,a)  \end{cases}
$$
根据探索策略（行动策略）和评估的策略是否为同一个策略，蒙特卡洛方法又分为 on-policy 和 off-policy 两种方法。若⾏动策略和评估及改善的策略是同⼀个策略，我们称为 on-policy ，可翻译为同策略。若⾏动策略和评估及改善的策略是不同的策略，我们称为 off-policy ，可翻译为异策略。

（1）同策略。

同策略（on-policy）是指产⽣数据的策略与评估和要改善的策略是同⼀个策略。⽐如，要产⽣数据的策略和评估及要改善的策略都是 $\epsilon-soft$ 策略。其伪代码如下所示。

![](.\3.png)

 

（2）异策略。

异策略（off-policy）是指产⽣数据的策略与评估和改善的策略不是同⼀个策略。我们用 $\pi$ 表示来评估和改善的策略，用 $\mu$ 表示产⽣样本数据的策略。异策略可以保证充分的探索性。例如用来评估和改善的策略 $\pi$ 是贪婪策略，用于产生数据的探索性策略 $\mu$  为探索性策略。

用于异策略的目标策略 $\pi$ 和行动策略 $\mu$ 并非任意选择的，而是必须满足一定的条件。这个条件是覆盖性条件，即行动策略 $\mu$ 产生的行为覆盖或包含目标策略 $\pi$ 产生的行为。利用式子表示：满足 $\pi(a|s) > 0$ 的任何 $(s,a)$ 均满足 $\mu(a|s) > 0$ 。

利用行为策略产生的数据评估目标策略需要利用重要性采样方法。下面描述重要性采样的原理。

![](.\4.png)

重要性采样来源于求期望，即 $E[f] = \int f(z)p(z)dz$ 。

当随机变量 $z$ 的分布非常复杂时，无法利用解析的方法产生用于逼近期望的样本，这时，我们可以选用一个很简单，很容易产生样本的概率分布 $q(z)$ ，比如正态分布。原来的期望可变为
$$
E[f] = \int f(z)p(z) dz \\
	 = \int f(z) \frac{p(z)}{q(z)} q(z) dz \\
	 \approx \frac{1}{N} \sum_n \frac{p(z^n)}{q(z^n)} f(z^n), z^n \sim q(z)
$$
定义重要性权重： $w^n = p(z^n) / q(z^n)$ ，普通的重要性采样求积分如下述方程所示为
$$
E[f] = \frac{1}{N} \sum_n w^n f(z^n)
$$
由式（8）可知，基于重要性采样的积分估计为无偏估计，即估计的期望值等于真实的期望。但是，基于重要性采样的积分估计的方差无穷大。这是因为原来的被积函数乘了一个重要性权重，改变了被积函数的形状及分布。尽管被积函数的均值没有发生变化，但方差明显发生改变。

一种减小重要性采样积分方差的方法是采用加权重要性采样：
$$
E[f] \approx \sum^{N}_{n=1} \frac{w^n}{\sum^{N}_{m=1} w^m} f(z^n)
$$
在异策略方法中，行动策略 $\mu$ 即用来产生样本的策略，所产生的轨迹概率分布相当于重要性采样中的 $q[z]$ ，用来评估和改进的策略 $\pi$ 所对应的轨迹概率分布为 $p[z]$ ，因此利用行动策略 $\mu$ 所产生的累积函数返回值来评估策略 $\pi$ 时，需要在累积函数返回值前面乘以重要性权重。

在目标策略 $\pi$ 下，一次试验的概率为
$$
Pr(A_t,S_{t+1},\cdots,S_T) = \prod^{T-1}_{k=t} \pi(A_k|S_k)p(S_{k+1}|S_k,A_k)
$$
在行动策略 $\mu$ 下，相应的试验的概率为
$$
Pr(A_t,S_{t+1},\cdots,S_T) = \prod^{T-1}_{k=t} \mu(A_k|S_k)p(S_{k+1}|S_k,A_k)
$$
因此，重要性权重为
$$
\rho^T_t = \frac{\prod^{T-1}_{k=t} \pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod^{T-1}_{k=t} \mu(A_k|S_k)p(S_{k+1}|S_k,A_k)} = \prod^{T-1}_{k=t} \frac{\pi(A_k|S_k)}{\mu(A_k|S_k)}
$$
普通重要性采样的值函数估计如下图所示。
$$
V(s) = \frac{\sum_{t \in \tau(s)} \rho^{T(t)}_{t} G_t}{|\tau(s)|}
$$
![](.\5.png)

其中， $t$ 是状态 $s$ 访问的时刻， $T(t)$ 是访问状态 $s$ 相对应的试验的终止状态所对应的时刻。 $\tau(s)$ 是状态 $s$ 发生的所有时刻集合。

加权重要性采样值函数估计为
$$
V(s) = \frac{\sum_{t \in \tau(s)} \rho^{T(t)}_{t} G_t}{\sum_{t \in \tau(s)} \rho^{T(t)}_t}
$$
那么，异策略每次访问蒙特卡洛算法的伪代码如下所示。

![](.\6.png)

### 3.2 统计学基础知识

总体：包含所研究的全部数据的集合。

样本：从总体中抽取的一部分元素的集合。在强化学习中，一个样本是指一个 episode 。
