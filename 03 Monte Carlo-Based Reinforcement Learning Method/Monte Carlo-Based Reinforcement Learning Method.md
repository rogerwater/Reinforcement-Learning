## 3 Monte Carlo-Based Reinforcement Learning Method

### 3.1 基于蒙特卡洛方法的理论

强化学习算法的精髓之一是解决无模型的马尔科夫决策问题。基于值函数的无模型的强化学习算法主要包括蒙特卡洛方法和时间差分方法。

对于模型已知的马尔科夫决策过程，可以采用动态规划的方法解决，即策略迭代和值迭代。这两种方法可以用广义策略迭代方法统一：即先进行策略评估，也就是计算当前策略所对应的值函数，再利用值函数改进当前策略。无模型的强化学习基本思想也是如此，即：策略评估和策略改善。

在动态规划的方法中，值函数的计算公式为
$$
\nu_{\pi}(s) = \sum_{a \in A}\pi(a|s)(R^a_s + \gamma\sum_{s' \in S} P^a_{SS'}\nu_{\pi}(s'))
$$
动态规划方法计算状态 $s$ 处的值函数利用了模型 $P^a_{SS'}$ ，而在无模型强化学习中，模型 $P^a_{SS'}$ 是未知的。⽆模型的强化学习算法要想利⽤策略评估和策略改善的框架，必须采⽤其他的⽅法评估当前策略（计算值函数）。

我们回到值函数最原始的定义公式：
$$
\nu_{\pi}(s) = E_{\pi}[G_t|S_t=s] = E_{\pi}[\sum^{\infty}_{k=0}\gamma^k R_{t+k+1} | S_t=s]
$$

$$
q_{\pi}(s) = E_{\pi}[\sum^{\infty}_{k=0}\gamma^k R_{t+k+1} | S_t=s,A_t=a]
$$

状态值函数和⾏为值函数的计算实际上是计算返回值的期望，动态规划的⽅法是利⽤模型计算该期望。在没有模型时，我们可以采⽤蒙特卡洛的⽅法计算该期望，即利⽤随机样本估计期望。在计算值函数时，蒙特卡洛⽅法是利⽤**经验平均**代替随机变量的期望。此处，我们要理解两个词：经验和平均。

**什么是“经验”**

当要评估智能体的当前策略 $\pi$ 时，我们可以利用策略 $\pi$ 产生很多次试验，每次试验都是从任意的初始状态开始直到终止，比如一次试验为 $S_1,A_1,R_2,\cdots,S_T$ ，计算一次试验中状态 $s$ 处的折扣回报返回值为 $G_t(s) = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1} R_T$ ，那么经验就是指利用该策略做很多次试验，产生很多幕数据。如下图所示。

![](.\1.png)

**什么是“平均”**

平均就是求平均值。 利⽤蒙特卡罗⽅法求状态 $s$ 处的值函数时，⼜可以分为第⼀次访问蒙特卡洛⽅法和每次访问蒙特卡洛⽅法。

第一次访问蒙特卡洛方法是指在计算状态 $s$ 处的值函数时，只利用每次试验中第一次访问到状态 $s$ 时的返回值。如上图中第一次试验所示，计算状态 $s$ 处的均值只利用 $G_{11}$ ，因此，第一次访问蒙特卡洛方法的计算公式为
$$
\nu(s) = \frac{G_{11}(s) + G_{21}(s) + \cdots}{N(s)}
$$
每次访问蒙特卡洛方法是指在计算状态 $s$ 处的值函数，利用所有访问到状态 $s$ 时的回报返回值，即
$$
\nu(s) = \frac{G_{11}(s) + G_{12}(s) + \cdots + G_{21}(s) + \cdots}{N(s)}
$$
根据大数定律： $\nu(s) \to \nu_{\pi}(s)\ \text{as} \ N(s) \to \infty$ 。

由于智能体与环境交互的模型是未知的，蒙特卡洛⽅法是利⽤经验平均来估计值函数，⽽能否得到正确的值函数，则取决于经验——因此，如何获得充⾜的经验是⽆模型强化学习的核⼼所在。

在动态规划⽅法中，为了保证值函数的收敛性，算法会逐个扫描状态空间中的状态。⽆模型的⽅法充分评估策略值函数的前提是每个状态都能被访问到，因此，在蒙特卡洛⽅法中必须采⽤⼀定的⽅法保证每个状态都能被访问到，⽅法之⼀是**探索性初始化**。

探索性初始化是指每个状态都有一定几率作为初始状态。在学习基于探索性初始化的蒙特卡洛方法前，需要先了解策略改善方法，以及便于进行迭代计算的平均方法。

**（1）蒙特卡洛策略改善。**

蒙特卡洛方法利用经验平均估计策略值函数。估计出值函数后，对于每个状态，它通过最大化动作值函数来进行策略的改善。即 $\pi(s) = \arg \underset{a}{\text{max}} \ q(s,a)$ 。

**（2）递增计算均值的方法如下式所示。**
$$
\nu_{k}(s) = \frac{1}{k} \sum^{k}_{j=1}G_j(s) \\
		   = \frac{1}{k} (G_k(s)) + \sum^{k-1}_{j=1} G_j(s) \\
		   = \frac{1}{k} (G_k(s) + (k-1)\nu_{k-1}(s)) \\
		   = \nu_{k-1}(s) + \frac{1}{k}(G_k(s) - \nu_{k-1}(s))
$$
下图即为探索性初始化蒙特卡洛方法的伪代码。

![](.\2.png)





