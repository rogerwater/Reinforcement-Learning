## 3 Monte Carlo-Based Reinforcement Learning Method

### 3.1 基于蒙特卡洛方法的理论

强化学习算法的精髓之一是解决无模型的马尔科夫决策问题。基于值函数的无模型的强化学习算法主要包括蒙特卡洛方法和时间差分方法。

对于模型已知的马尔科夫决策过程，可以采用动态规划的方法解决，即策略迭代和值迭代。这两种方法可以用广义策略迭代方法统一：即先进行策略评估，也就是计算当前策略所对应的值函数，再利用值函数改进当前策略。无模型的强化学习基本思想也是如此，即：策略评估和策略改善。

在动态规划的方法中，值函数的计算公式为
$$
\nu_{\pi}(s) = \sum_{a \in A}\pi(a|s)(R^a_s + \gamma\sum_{s' \in S} P^a_{SS'}\nu_{\pi}(s'))
$$
动态规划方法计算状态 $s$ 处的值函数利用了模型 $P^a_{SS'}$ ，而在无模型强化学习中，模型 $P^a_{SS'}$ 是未知的。⽆模型的强化学习算法要想利⽤策略评估和策略改善的框架，必须采⽤其他的⽅法评估当前策略（计算值函数）。

我们回到值函数最原始的定义公式：
$$
\nu_{\pi}(s) = E_{\pi}[G_t|S_t=s] = E_{\pi}[\sum^{\infty}_{k=0}\gamma^k R_{t+k+1} | S_t=s]
$$

$$
q_{\pi}(s) = E_{\pi}[\sum^{\infty}_{k=0}\gamma^k R_{t+k+1} | S_t=s,A_t=a]
$$

状态值函数和⾏为值函数的计算实际上是计算返回值的期望，动态规划的⽅法是利⽤模型计算该期望。在没有模型时，我们可以采⽤蒙特卡洛的⽅法计算该期望，即利⽤随机样本估计期望。在计算值函数时，蒙特卡洛⽅法是利⽤**经验平均**代替随机变量的期望。此处，我们要理解两个词：经验和平均。

**什么是“经验”**

当要评估智能体的当前策略 $\pi$ 时，我们可以利用策略 $\pi$ 产生很多次试验，每次试验都是从任意的初始状态开始直到终止，比如一次试验为 $S_1,A_1,R_2,\cdots,S_T$ ，计算一次试验中状态 $s$ 处的折扣回报返回值为 $G_t(s) = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1} R_T$ ，那么经验就是指利用该策略做很多次试验，产生很多幕数据。如下图所示。

![](.\1.png)

**什么是“平均”**

平均就是求平均值。 利⽤蒙特卡罗⽅法求状态 $s$ 处的值函数时，⼜可以分为第⼀次访问蒙特卡洛⽅法和每次访问蒙特卡洛⽅法。

第一次访问蒙特卡洛方法是指在计算状态 $s$ 处的值函数时，只利用每次试验中第一次访问到状态 $s$ 时的返回值。如上图中第一次试验所示，计算状态 $s$ 处的均值只利用 $G_{11}$ ，因此，第一次访问蒙特卡洛方法的计算公式为
$$
\nu(s) = \frac{G_{11}(s) + G_{21}(s) + \cdots}{N(s)}
$$
每次访问蒙特卡洛方法是指在计算状态 $s$ 处的值函数，利用所有访问到状态 $s$ 时的回报返回值，即
$$
\nu(s) = \frac{G_{11}(s) + G_{12}(s) + \cdots + G_{21}(s) + \cdots}{N(s)}
$$
根据大数定律： $\nu(s) \to \nu_{\pi}(s)\ \text{as} \ N(s) \to \infty$ 。

由于智能体与环境交互的模型是未知的，蒙特卡洛⽅法是利⽤经验平均来估计值函数，⽽能否得到正确的值函数，则取决于经验——因此，如何获得充⾜的经验是⽆模型强化学习的核⼼所在。

在动态规划⽅法中，为了保证值函数的收敛性，算法会逐个扫描状态空间中的状态。⽆模型的⽅法充分评估策略值函数的前提是每个状态都能被访问到，因此，在蒙特卡洛⽅法中必须采⽤⼀定的⽅法保证每个状态都能被访问到，⽅法之⼀是**探索性初始化**。

探索性初始化是指每个状态都有一定几率作为初始状态。在学习基于探索性初始化的蒙特卡洛方法前，需要先了解策略改善方法，以及便于进行迭代计算的平均方法。

**（1）蒙特卡洛策略改善。**

蒙特卡洛方法利用经验平均估计策略值函数。估计出值函数后，对于每个状态，它通过最大化动作值函数来进行策略的改善。即 $\pi(s) = \arg \underset{a}{\text{max}} \ q(s,a)$ 。

**（2）递增计算均值的方法如下式所示。**
$$
\nu_{k}(s) = \frac{1}{k} \sum^{k}_{j=1}G_j(s) \\
		   = \frac{1}{k} (G_k(s)) + \sum^{k-1}_{j=1} G_j(s) \\
		   = \frac{1}{k} (G_k(s) + (k-1)\nu_{k-1}(s)) \\
		   = \nu_{k-1}(s) + \frac{1}{k}(G_k(s) - \nu_{k-1}(s))
$$
下图即为探索性初始化蒙特卡洛方法的伪代码。

![](.\2.png)

探索性初始化在迭代每⼀幕时，初始状态是随机分配的，这样可以保证迭代过程中每个状态⾏为对都能被选中。它蕴含着⼀个假设：假设所有的动作都被⽆限频繁选中。对于这个假设，有时很难成⽴，或⽆法完全保证。如何保证在初始状态不变的同时，⼜能保证每个状态⾏为对可以被访问到？

精心设计探索策略。策略必须是温和的，即对所有的状态 $s$ 和 $a$ 满足： $\pi(a|s)>0$ 。也就是说，温和的探索策略是指在任意状态下，采用动作集中每个动作的概率都大于零。典型的温和策略是 $\epsilon-soft$ 策略：
$$
\pi(a|s) \gets \begin{cases} 1-\epsilon+\frac{\epsilon}{|A(s)|} & if \ a = \arg  max_aQ(s,a) \\
\frac{\epsilon}{|A(s)|} & if \ a \neq \arg max_aQ(s,a)  \end{cases}
$$
根据探索策略（行动策略）和评估的策略是否为同一个策略，蒙特卡洛方法又分为 on-policy 和 off-policy 两种方法。若⾏动策略和评估及改善的策略是同⼀个策略，我们称为 on-policy ，可翻译为同策略。若⾏动策略和评估及改善的策略是不同的策略，我们称为 off-policy ，可翻译为异策略。

（1）同策略。

同策略（on-policy）是指产⽣数据的策略与评估和要改善的策略是同⼀个策略。⽐如，要产⽣数据的策略和评估及要改善的策略都是 $\epsilon-soft$ 策略。其伪代码如下所示。

![](.\3.png)

 

（2）异策略。

异策略（off-policy）是指产⽣数据的策略与评估和改善的策略不是同⼀个策略。我们用 $\pi$ 表示来评估和改善的策略，用 $\mu$ 表示产⽣样本数据的策略。异策略可以保证充分的探索性。例如用来评估和改善的策略 $\pi$ 是贪婪策略，用于产生数据的探索性策略 $\mu$  为探索性策略。

用于异策略的目标策略 $\pi$ 和行动策略 $\mu$ 并非任意选择的，而是必须满足一定的条件。这个条件是覆盖性条件，即行动策略 $\mu$ 产生的行为覆盖或包含目标策略 $\pi$ 产生的行为。利用式子表示：满足 $\pi(a|s) > 0$ 的任何 $(s,a)$ 均满足 $\mu(a|s) > 0$ 。

利用行为策略产生的数据评估目标策略需要利用重要性采样方法。下面描述重要性采样的原理。

![](.\4.png)

重要性采样来源于求期望，即 $E[f] = \int f(z)p(z)dz$ 。

当随机变量 $z$ 的分布非常复杂时，无法利用解析的方法产生用于逼近期望的样本，这时，我们可以选用一个很简单，很容易产生样本的概率分布 $q(z)$ ，比如正态分布。原来的期望可变为
$$
E[f] = \int f(z)p(z) dz \\
	 = \int f(z) \frac{p(z)}{q(z)} q(z) dz \\
	 \approx \frac{1}{N} \sum_n \frac{p(z^n)}{q(z^n)} f(z^n), z^n \sim q(z)
$$
定义重要性权重： $w^n = p(z^n) / q(z^n)$ ，普通的重要性采样求积分如下述方程所示为
$$
E[f] = \frac{1}{N} \sum_n w^n f(z^n)
$$
由式（8）可知，基于重要性采样的积分估计为无偏估计，即估计的期望值等于真实的期望。但是，基于重要性采样的积分估计的方差无穷大。这是因为原来的被积函数乘了一个重要性权重，改变了被积函数的形状及分布。尽管被积函数的均值没有发生变化，但方差明显发生改变。

一种减小重要性采样积分方差的方法是采用加权重要性采样：
$$
E[f] \approx \sum^{N}_{n=1} \frac{w^n}{\sum^{N}_{m=1} w^m} f(z^n)
$$
在异策略方法中，行动策略 $\mu$ 即用来产生样本的策略，所产生的轨迹概率分布相当于重要性采样中的 $q[z]$ ，用来评估和改进的策略 $\pi$ 所对应的轨迹概率分布为 $p[z]$ ，因此利用行动策略 $\mu$ 所产生的累积函数返回值来评估策略 $\pi$ 时，需要在累积函数返回值前面乘以重要性权重。

在目标策略 $\pi$ 下，一次试验的概率为
$$
Pr(A_t,S_{t+1},\cdots,S_T) = \prod^{T-1}_{k=t} \pi(A_k|S_k)p(S_{k+1}|S_k,A_k)
$$
在行动策略 $\mu$ 下，相应的试验的概率为
$$
Pr(A_t,S_{t+1},\cdots,S_T) = \prod^{T-1}_{k=t} \mu(A_k|S_k)p(S_{k+1}|S_k,A_k)
$$
因此，重要性权重为
$$
\rho^T_t = \frac{\prod^{T-1}_{k=t} \pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod^{T-1}_{k=t} \mu(A_k|S_k)p(S_{k+1}|S_k,A_k)} = \prod^{T-1}_{k=t} \frac{\pi(A_k|S_k)}{\mu(A_k|S_k)}
$$
普通重要性采样的值函数估计如下图所示。
$$
V(s) = \frac{\sum_{t \in \tau(s)} \rho^{T(t)}_{t} G_t}{|\tau(s)|}
$$
![](.\5.png)

其中， $t$ 是状态 $s$ 访问的时刻， $T(t)$ 是访问状态 $s$ 相对应的试验的终止状态所对应的时刻。 $\tau(s)$ 是状态 $s$ 发生的所有时刻集合。

加权重要性采样值函数估计为
$$
V(s) = \frac{\sum_{t \in \tau(s)} \rho^{T(t)}_{t} G_t}{\sum_{t \in \tau(s)} \rho^{T(t)}_t}
$$
那么，异策略每次访问蒙特卡洛算法的伪代码如下所示。

![](.\6.png)

### 3.2 统计学基础知识

**总体：**包含所研究的全部数据的集合。

**样本：**从总体中抽取的一部分元素的集合。在强化学习中，一个样本是指一个 episode 。

**统计量：**用来描述样本特征的概括性数字度量。如样本均值，样本方差，样本标准差等。在强化学习中，我们用样本均值衡量状态值函数。

**样本均值：**设 $X_1,X_2,\cdots,X_n$ 为样本容量为 $n$ 的随机样本，它们是独立同分布的随机变量，则样本均值为
$$
\bar{X} = \frac{X_1 + X_2 + \cdots + X_n}{n}
$$
样本均值也是随机变量。

**样本方差：**设 $X_1,X_2,\cdots,X_n$ 为样本容量为 $n$ 的随机样本，它们是独立同分布的随机变量，则样本方差为
$$
\hat{S}^2 = \frac{(X_1 - \bar{X})^2 + (X_2 - \bar{X})^2 + \cdots + (X_n - \bar{X})^2}{n}
$$
**无偏估计：**若样本的统计量等于总体的统计量，则称该样本的统计量所对应的值为无偏估计。如总体的均值和方差分别为 $\mu$ 和 $\sigma^2$ 时，若 $E(\bar{X}) = \mu$ ， $E(\hat{S}^2) = \sigma^2$ ，则 $\bar{X}$ 和 $\hat{S}^2$ 称为无偏估计。

**蒙特卡洛积分与随机采样方法：**

蒙特卡洛方法常用来计算函数的积分，如计算下式积分。
$$
\int_a^b f(x) dx
$$
如果 $f(x)$ 的函数形式非常复杂，则只能利用数值的方法计算：取很多样本点，计算 $f(x)$ 在这些样本点处的值。上式等价变换为
$$
\int_a^b \frac{f(x)}{\pi(x)} \pi(x) dx
$$
其中 $\pi(x)$ 为已知的分布。

（1）取样本点：可以根据 $\pi(x)$ 进行随机采样，得到采样点。

（2）求平均：根据分布 $\pi(x)$ 采样 $x_i$ 后，在样本点处计算 $\frac{f(x_i)}{\pi(x_i)}$ ，并对所有样本点处的值求均值：
$$
\frac{1}{n} \sum_i \frac{f(x_i)}{\pi(x_i)}
$$
以上为蒙特卡洛方法计算积分的原理。

对于期望的计算，设 $X$ 表示随机变量，且服从概率分布 $\pi(x)$ ，计算函数 $g(x)$ 的期望。函数 $g(x)$ 期望的计算公式为
$$
\int g(x)\pi(x) dx
$$
当目标分布 $\pi(x)$ 非常复杂或未知时，需要利用统计学中的各种采样技术。常见的采样方法有两类。第一类是指定一个已知的概率分布 $p(x)$ 用于采样，指定的采样概率分布称为提议分布。这类采样方法包括拒绝采样和重要性采样。此类方法只适用于低维情况，针对高维情况常采用第二类采样方法，即马尔科夫链蒙特卡洛的方法。该方法的基本原理是从平稳分布为 $\pi$ 的马尔科夫链中产生非独立样本。

（1）拒绝采样。

当目标分布 $\pi(x)$ 非常复杂或未知时，无法利用目标分布给出采样点，则可以采用一个易于采样的提议分布 $p(x)$ ，如高斯分布进行采样。为了得到符合目标分布 $\pi(x)$ 的样本，需要加工由提议分布 $p(x)$ 得到的样本，接受符合目标分布的样本，拒绝不符合目标分布的样本。

（2）重要性采样。

（3）MCMC 方法。

MCMC 采样的方法原理与拒绝采样、重要性采样的原理有本质的区别。拒绝采样和重要性采样利用提议分布产生采样点，当维数很高时，难以找到合适的提议分布，采样效率差。MCMC 的方法不需要提议分布，只需要一个随机样本点，下一个样本会由当前的随机样本点产生，如此循环源源不断地产生很多样本点。最终，这些样本点服从目标分布。

其背后的定理是：目标分布为马尔科夫链平稳分布，即该目标分布存在一个转移概率矩阵 $P$ ，且该转移概率满足：
$$
\pi(j) = \sum_{i=0}^{\infty} \pi(i) P_{ij} \quad ; \quad \pi是方程\pi P = \pi 的唯一非负解
$$
当转移矩阵 $P$ 满足上述条件时，从任意初始分布 $\pi_0$ 出发，经过一段时间迭代，分布 $\pi_t$ 都会收敛到目标分布 $\pi$ 。因此，假设我们已知满足条件的状态转移概率矩阵 $P$ ，那么对于任意一个初始状态 $x_0$ ，则可以得到一个转移序列 $x_0,x_1,\cdots,x_n,x_{n+1},\cdots$ 。如果该马尔科夫链在第 $n$ 步已经收敛到目标分布 $\pi$ ，那么我们就得到了服从目标分布 $\pi$ 的样本 $x_n,x_{n+1},\cdots$ 。

转移概率 $P$ 和分布应该满足细致平稳条件，即
$$
\pi(i) P_{ij} = \pi(j) P_{ji} \quad for \ all \ i,j
$$
为构造转移概率，加入一个已有的一个转移矩阵为 $Q$ 的马尔科夫链，一般情况下，其不满足细致平稳条件，即
$$
p(i) q(i,j) \neq p(j) q(j,i)
$$
通过加入一项 $\alpha(i,j)$ ，使其满足细致平稳条件，即
$$
p(i) q(i,j) \alpha(i,j) = p(j) q(j,i) \alpha(j,i)
$$
$\alpha(i,j)$ 被称为接受率，可按下述方式取值：
$$
\alpha(i,j) = p(j) q(j,i) , \alpha(j,i) = p(i) q(i,j)
$$
综上所述，MCMC 方法可以总结为以下步骤。

1. 初始化马尔科夫链初始状态 $X_0 = x_0$ ；
2. 对 $t=0,1,2,\cdots$ ，循环以下 3-6 步，不断采样：
3. 第 $t$ 时刻的马尔科夫链状态为 $X_t = x_t$ ，采样 $y \sim q(x|x_t)$ ；
4. 从均匀分布中采样 $u \sim Uniform[0,1]$ ；
5. 如果 $u < \alpha(x_t,y) = p(y) q(x_t|y)$ ，则接受转移 $x_t \to y$ ，即下一时刻的状态 $X_{t+1} = y$ ；
6. 否则不接受转移，即 $X_{t+1} = x_t$ 。

为了提高接受率，使得样本多样化，第五步可以改写为
$$
\alpha(x_t,y) = min\{ \frac{p(y)q(x_t|y)}{p(x_t)q(y|x_t)}, 1 \}
$$
采样这种接受率的算法称为 Metropolis-Hastings 算法。

