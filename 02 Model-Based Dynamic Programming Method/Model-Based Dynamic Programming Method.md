## 2 Model-Based Dynamic Programming Method

### 2.1 基于模型的动态规划方法理论

一个完整的已知模型的马尔可夫决策过程可以利用元组 $(S,A,P,r,\gamma)$ 来表示。其中 $S$ 为状态集， $A$ 为动作集， $P$ 为转移概率，也就是对应着环境和智能体的模型， $r$ 为回报函数， $\gamma$ 为折扣因子，用来计算累积回报 $R$ 。累积回报公式为 $R = \sum^{T}_{t=0} \gamma^t r_t$ ，其中 $0 \le \gamma  \le 1$ ，当 $T$ 为有限值时，强化学习过程称为有限范围强化学习；当 $T = \infty$ 时，称为无限范围强化学习。 

强化学习的目标是找到最优策略 $\pi$ 使得累积回报的期望最大。所谓策略是指状态到动作的映射： $\pi : s \to a$ ，用 $\tau$ 表示从状态 $s$ 到最终状态的一个序列 $\tau : s_t, s_{t+1}, \cdots, s_T$ ，则累积回报 $R(\tau)$ 是一个随机变量，随机变量无法进行优化，无法作为目标函数，我们采用随机变量的期望作为目标函数，即 $\int R(\tau) p_{\pi}(\tau) d\tau$ 作为目标函数。用公式来表示强化学习的目标： $\underset{\pi}{\text{max}} \ \int R(\tau) p_{\pi}(\tau) d\tau$ 。强化学习的最终目标是找到最优策略为 $\pi^* : s \to u^*$ 。

从广义上来讲，强化学习可以归结为序贯决策问题，即找到一个决策序列，使得目标函数最优。这里目标函数是累积回报的期望值，累积回报的含义是评价策略完成任务的总回报，所以目标函数等价于任务。强化学习的直观目标是找到最优策略，目的是更好地完成任务。回报函数对应着具体的任务，所以强化学习所学到的最优策略是与具体的任务相对应的。从这个意义上来说，强化学习并不是万能的，它无法利用一个算法实现所有的任务。

![](\1.png)

基于模型的强化学习可以利用动态规划的思想来解决。利用动态规划可以解决的问题需要满足两个条件：一是整个优化问题可以分解为多个子优化问题；二是子优化问题的解可以被存储和重复利用。强化学习可以利用马尔科夫决策过程来描述，利用贝尔曼最优性原理得到贝尔曼最优方程：
$$
\nu^*(s) = \underset{a}{\text{max}} \ R^a_s + \gamma \sum_{s' \in S} P^a_{SS'} \nu^*(s') \\
q^*(s,a) = R^a_s + \gamma \sum_{s' \in S}  P^a_{SS'} \text{max}_{a'} \ q^*(s',a')
$$
从上式可以看出，马尔科夫决策问题符合使用动态规划的两个条件，因此可以用动态规划解决马尔科夫决策过程的问题。





