##  1 Markov Decision Process

### 1.1 马尔科夫决策过程理论讲解

#### 1.1.1 马尔科夫性

马尔科夫性：状态 $s_t$ 是马尔科夫的，当且仅当 $P[s_{t+1}|s_t] = P[s_{t+1}|s_1,...,s_t]$ 。

马尔科夫性是指系统的下一个状态 $s_{t+1}$ 仅与当前状态 $s_{t}$ 有关，而与以前的状态无关。

随机过程：指随机变量序列。

马尔科夫随机过程：随机变量序列中的每个状态都是马尔科夫的。

#### 1.1.2 马尔科夫过程

马尔科夫过程：马尔科夫过程是一个二元组 $(S, P)$ ，且满足： $S$ 是有限状态集合， $P$  是状态转移概率。状态转移概率矩阵为 $\begin{bmatrix} P_{11} & \cdots & P_{1n} \\ \vdots & \vdots & \vdots \\ P_{n1} & \cdots & P_{nn}  \end{bmatrix}$  。

状态序列称为马尔科夫链，当给定状态转移概率时，从某个状态出发存在多条马尔科夫链。

#### 1.1.3 马尔科夫决策过程

马尔科夫决策过程：将动作（策略）和回报考虑在内的马尔科夫过程。

马尔科夫决策过程由元组 $(S, A, P, R, \gamma)$  描述，其中：

$S$	为有限的状态集

$A$	为有限的动作集

$P$	为状态转移概率

$R$	为回报函数

$\gamma$	为折扣因子，用来计算累积回报

强化学习的目标是给定一个马尔科夫决策过程，寻找最优策略。所谓策略是指状态到动作的映射，策略通常用符号 $\pi$ 表示，它是指给定状态 $s$ 时，动作集上的一个分布，即

$$
\pi(a | s) = p[A_t = a|S_t = s]
$$

即策略的定义是用条件概率分布给出的。公式（ $1$ ）的含义是：策略 $\pi$ 在每个状态 $s$ 指定一个动作概率。如果给出的策略 $\pi$ 是确定性的，那么策略 $\pi$ 在每个状态 $s$ 指定一个确定的动作。

当给定一个策略 $\pi$ 时，就可以计算累积回报。首先定义累积回报： 

$$
G_t = R_{t+1} + \gamma R_{t+2} + \cdots = \sum^{\infty}_{k=0} \gamma^k R_{t+k+1}
$$

由于策略 $\pi$ 是随机的，因此累积回报也是随机的。为了评价状态 $s_1$ 的价值，我们需要定义一个确定量来描述状态 $s_1$ 的价值，很自然的想法是利用累积回报来衡量状态 $s_1$ 的价值。然而，累积回报 $G_1$ 是一个随机变量，不是一个确定值，因此无法描述，但其期望是个确定值，可以作为状态值函数的定义。

（1）状态值函数。

当智能体采用策略 $\pi$ 时，累积回报服从一个分布，累积回报在状态 $s$ 处的期望值定义为状态-值函数： 

$$
\nu_{\pi}(s) = E_{\pi}[\sum^{\infty}_{k=0} \gamma^k R_{t+k+1} | S_t = s]
$$


注意：状态值函数是与策略 $\pi$ 相对应的，这是因为策略 $\pi$ 决定了累积回报 $G$ 的状态分布。

相应地，状态-行为值函数为 

$$
q_{\pi}(s,a) = E_{\pi}[\sum^{\infty}_{k=0} \gamma^k R_{t+k+1} | S_t = s, A_t = a]
$$

（2）状态值函数与状态-行为值函数的贝尔曼方程。

由状态值函数的定义式（ $3$ ）可以得到： 

$$
\nu(s) = E[G_t | S_t = s] \\ 
	   = E[R_{t+1} + \gamma R_{t+2} + \cdots | S_t = s] \\
       = E[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \cdots) | S_t = s] \\
       = E[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
       = E[R_{t+1} + \gamma \nu(S_{t+1}) | S_t = s]
$$

同样我们可以得到状态-动作值函数的贝尔曼方程： 

$$
q_{\pi}(s, a) = E_{\pi}[R_{t+1} + \gamma q(S_{t+1},A_{t+1})|S_t = s, A_t = a]
$$
