## 5 Value Function Approximation-Based Reinforcement Learning Method

### 5.1 基于值函数逼近的理论分析

#### 5.1.1 值函数逼近理论

对于表格型值函数，可以采用前述的基于动态规划的方法，基于蒙特卡洛的方法和基于时间差分的方法。这些方法有一个基本的前提条件：状态空间和动作空间是离散的，而且状态空间和动作空间不能太大。对于状态值函数，其索引是状态；对于行为值函数，其索引是状态-行为对。值函数的迭代更新实际上就是这张表的迭代更新。若状态空间的维数很大，或者状态空间为连续空间，此时值函数无法用一张表格来表示。这时，我们需要利用函数逼近的方法表示值函数，如下图所示。当值函数利用函数逼近的方法表示后，可以利用策略迭代和值迭代方法构建强化学习算法。

![](.\1.png)

在表格型强化学习中，值函数对应着一张表。在值函数逼近方法中，值函数对应着一个逼近函数 $\hat{v}(s)$ 。从数学角度看，函数逼近方法可以分为参数逼近和非参数逼近。其中参数逼近又分为线性化参数逼近和非线性化参数逼近。

对于参数化逼近，是指值函数可以由一组参数 $\theta$ 来近似。我们将逼近的值函数写为 $\hat{v}(s,\theta)$ 。

当逼近的值函数结构确定时，值函数的逼近就等价于参数的逼近，值函数的更新也就等价与参数的更新。也就是说，我们需要利用试验数据来更新参数值。

对于表格型值函数的更新过程，都是朝着一个目标值更新的，这个目标值在蒙特卡洛方法中是 $G_t$ ，在时间差分方法中是 $r+\gamma Q(s',a')$ ，在 $TD(\lambda)$ 中是 $G^{\lambda}_t$ 。

将表格型强化学习值函数的更新过程推广到值函数逼近过程，有如下形式。

函数逼近 $\hat{v}(s,\theta)$ 的过程是一个监督学习的过程，其数据和标签对为 $(S_t,U_t)$ ，其中 $U_t$ 等价于表格型值函数的目标值。

训练的目标函数为
$$
\arg \underset{\theta}{\text{min}} \ (q(s,a) - \hat{q}(s,a,\theta))^2
$$
下面我们比较总结一下表格型强化学习和函数逼近方法的强化学习值函数更新时的异同点。

（1）表格型强化学习在更新值函数时，只有当前状态 $S_t$ 处的值函数改变，其他地方的值函数不改变。

（2）值函数逼近方法更新值函数时，更新的是参数 $\theta$ ，而估计的值函数为 $\hat{v}(s,\theta)$ ，所以当参数 $\theta$ 发生改变，任意状态处的值函数都会发生改变。

#### 5.1.2 值函数更新方法

值函数更新可以分为增量式学习方法和批学习方法。

对于增量式学习方法，随机梯度下降法是最常用的增量式学习方法。

**增量式学习方法：随机梯度下降法**

由（1）式我们可以得到参数的随机梯度更新为：
$$
\theta_{t+1} = \theta_{t} + \alpha [U_t - \hat{v}(S_t,\theta_t)] \nabla_{\theta} \hat{v}(S_t,\theta)
$$
**基于蒙特卡洛方法的函数逼近**：

给定要评估的策略 $\pi$ ，产生一次试验：

![](.\2.png)

值函数的更新过程实际是一个监督学习的过程，其中监督数据集从蒙特卡洛的试验中得到，数据集为 $<s_1,G_1>$， $<s_2,G_2>$ ， $\cdots$ ， $<s_T,G_T>$ 。

值函数的更新如下。
$$
\Delta\theta = \alpha(G_t - \hat{v}(S_t,\theta)) \nabla_{\theta} \hat{v}(S_t,\theta)
$$
下图为基于梯度的蒙特卡洛值函数逼近更新过程。蒙特卡洛方法的目标值函数使用一次试验的整个回报返回值。

![](.\3.png)

**基于时间差分方法的函数逼近**

$TD(0)$ 方法中目标值函数为 $U_t = R_{t+1} + \gamma \hat{v}(S_{t+1},\theta)$ ，即目标值函数用到了 bootstrapping 的方法。

此时要更新的参数 $\theta$ 不仅出现在要估计的值函数 $\hat{v}(S_t,\theta)$ 中，还出现在目标值函数 $U_t$ 中。若只考虑参数 $\theta$ 对估计值函数 $\hat{v}(S_t,\theta)$ 的影响而忽略对目标值函数的影响，这种方法就不是完全的梯度法，因此也称为基于半梯度的 $TD(0)$ 值函数评估算法，如下图所示。
$$
\theta_{t+1} = \theta_t + \alpha [R + \gamma\hat{v}(S',\theta) - \hat{S_t,\theta_t}] \nabla \hat{v}(S_t,\theta_t)
$$
![](.\4.png)

下图为基于半梯度的 Sarsa 算法。与表格型强化学习相比，值函数逼近方法中把对值函数的更新换成了对参数的更新，参数的学习过程为监督学习。

![](.\5.png)

**值函数形式**

值函数可以采用线性逼近和非线性逼近，非线性逼近常用的是神经网络。

下面仅讨论线性逼近： $\hat{v}(s,\theta) = \theta^T \phi(s)$ 。

相比于非线性逼近，线性逼近的好处是只有一个最优值，因此可以收敛到全局最优。其中，$\phi(s)$ 为状态 $s$ 处的特征函数，或者称为基函数。

常用的基函数的类型如下。

多项式基函数，如 $(1,s_1,s_2,s_1 s_2,s^2_1,s^2_2，\cdots)$ 。

傅里叶基函数： $\phi_i(s) = cos(i \pi s), \ s \in [0,1]$ 。

径向基函数： $\phi_i(s) = \exp (- \frac{||s-c_i||^2}{2\sigma^2_i})$ 。

将线性逼近值函数带入随机梯度下降法和半梯度下降法中，可以得到参数的更新公式，不同强化学习方法更新公式如下。

蒙特卡洛方法值函数更新公式：
$$
\Delta \theta = \alpha [U_t(s) - \hat{v}(S_t,\theta_t)] \nabla\hat{v}(S_t,\theta_t) \\
= \alpha [G_t - \theta^T\phi]\phi
$$
$TD(0)$ 线性逼近值函数更新公式：
$$
\Delta\theta = \alpha [R+\gamma \theta^T\phi(s') - \theta^T\phi(s)]\phi(s) \\
= \alpha \delta \phi(s)
$$
正向视角的 $TD(\lambda)$ 更新公式：
$$
\Delta \theta = \alpha(G^{\lambda}_t - \theta^T\phi)\phi
$$
后向视角的 $TD(\lambda)$ 更新公式：
$$
\delta_t = R_{t+1} + \gamma\theta^T\phi(s') - \theta^T\phi(s) \\
E_t = \gamma \lambda E_{t-1} + \phi(s) \\
\Delta \theta = \alpha \delta_t E_t
$$
**批学习方法**

增量式方法参数更新过程随机性比较大，尽管计算简单，但样本数据的利用效率并不高。

批学习方法虽然计算复杂，但计算效率高。

所谓批的方法是指给定经验数据集 $D={<s_1,v^{\pi}_1>, }$

